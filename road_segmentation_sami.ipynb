{"cells":[{"cell_type":"markdown","source":["## Google Collab"],"metadata":{"id":"_NAW26QClLDb"}},{"cell_type":"code","source":["!lscpu |grep 'Model name'\n","!lscpu |grep 'Core(s) per socket:'\n","!free -h\n","!lscpu |grep 'Thread(s) per core'"],"metadata":{"id":"nkWcAK0plMU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639667774727,"user_tz":-60,"elapsed":830,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}},"outputId":"ac49e072-4ea6-4bf3-ef5f-da7aa64dc33c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Core(s) per socket:  2\n","              total        used        free      shared  buff/cache   available\n","Mem:            25G        868M         22G        1.2M        2.1G         24G\n","Swap:            0B          0B          0B\n","Thread(s) per core:  2\n"]}]},{"cell_type":"code","source":["# Mount Google Drive and load project 2\n","# WARNING, we have to add the shared drive of Sami on our home directory (by creating an alias)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#Move to the shared directory\n","%cd /content/drive/MyDrive/ml_project_2_drive/ml_project_2/\n","# list all files\n","! ls\n","\n","# Read helpers python file\n","!cp /content/drive/MyDrive/ml_project_2_drive/ml_project_2/script/helper_functions.py .\n","\n","# A good help can be found here:\n","#https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d"],"metadata":{"id":"9qxMlClOlMvy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639667768763,"user_tz":-60,"elapsed":2444,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}},"outputId":"32aa2931-40a3-4a0b-8af9-1fe46ccfe041"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/ml_project_2_drive/ml_project_2\n","data\t\t     project2_description.pdf  road_segmentation_sami.ipynb\n","helper_functions.py  README.md\t\t       script\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"sami.ferchiou@epfl.ch\"\n","!git config --global user.name \"samiferchiou\""],"metadata":{"id":"VECz4EgalRdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git pull"],"metadata":{"id":"7riEcnWTlUm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add .\n","!git commit -m \"update from google colab\"\n","!git push"],"metadata":{"id":"4wGL_9LQlU8r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OUp7rizSG5J-"},"source":["## library"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"f6Gz3ZmT6hYq","executionInfo":{"status":"ok","timestamp":1639667778529,"user_tz":-60,"elapsed":439,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["def load_image(infilename):\n","    data = mpimg.imread(infilename)\n","    return data\n","\n","def load_train_dataset():\n","    #root_dir = \"../data/training/\"\n","    root_dir = \"/content/drive/MyDrive/ml_project_2_drive/ml_project_2/data/training/\"\n","    image_dir = root_dir + \"images/\"\n","    gt_dir = root_dir + \"groundtruth/\"\n","    files = os.listdir(image_dir)\n","    n = len(files)\n","\n","    np_X = [load_image(image_dir + files[i]) for i in range(n)]\n","    np_Y = [load_image(gt_dir + files[i]) for i in range(n)]\n","\n","    torch_X = [torch.from_numpy(x).reshape(3, 400, 400) for x in np_X]\n","    torch_Y =[torch.from_numpy(y).reshape(1, 400, 400) for y in np_Y]\n","\n","    return (torch_X, torch_Y)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ObEjY926JWAz","executionInfo":{"status":"ok","timestamp":1639667780867,"user_tz":-60,"elapsed":237,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state:            checkpoint we want to save\n","    is_best:          boolean to indicates if it is the best checkpoint\n","    checkpoint_path:  path to save checkpoint\n","    best_model_path:  path to save best model\n","    \"\"\"\n","    torch.save(state, checkpoint_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        shutil.copyfile(checkpoint_path, best_fpath)"]},{"cell_type":"markdown","metadata":{"id":"Fgh7QOsWOITB"},"source":["## Imports"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"neDfYKPpOITJ","executionInfo":{"status":"ok","timestamp":1639667787588,"user_tz":-60,"elapsed":5877,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import ExponentialLR\n","import numpy as np\n","import random\n","import os,sys\n","from PIL import Image\n","import torchvision.transforms as T\n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","\n","#from helper_functions import *"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUAPLcgQOITR","outputId":"712e5811-f3ea-4c67-d251-69a266d36553","executionInfo":{"status":"ok","timestamp":1639667787589,"user_tz":-60,"elapsed":12,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"xdgnn_O4OITV"},"source":["## Constants definition"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"-0QkBAN5OITZ","executionInfo":{"status":"ok","timestamp":1639667787590,"user_tz":-60,"elapsed":11,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["CHECKPOINT_PATH =\"/content/drive/MyDrive/ml_project_2_drive/ml_project_2/checkpoint/current_checkpoint.pt\"\n","BEST_MODEL_PATH =\"/content/drive/MyDrive/ml_project_2_drive/ml_project_2/checkpoint/best_model.pt\"\n","NBR_EPOCHS = 5\n","BATCH_SIZE = 1\n","LEARNING_RATE = 1e-3\n","WEIGHT_DECAY = 0\n","GAMMA = 1\n","K_FOLD = 4\n","VALIDATION_SET_IDX = 0\n","BATCH_SIZE_VAL = 5\n","SEED = 0"]},{"cell_type":"markdown","metadata":{"id":"Q8je-hU0OITe"},"source":["## Data set"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"0U8H1A9IOITi","executionInfo":{"status":"ok","timestamp":1639667832647,"user_tz":-60,"elapsed":43641,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["class imagesDataset(Dataset): \n","    def __init__(self, K_fold, validation_set_idx, batch_size_val, seed):\n","        X, Y = load_train_dataset()\n","\n","        #tresholding ground_truth values\n","        Y = [(y > 0.5).long() for y in Y]\n","        shape_y = Y[0].shape\n","        \n","        #shuffling\n","        random.seed(seed)\n","        idx_list = list(range(len(X)))\n","        random.shuffle(idx_list)\n","        random.seed()\n","        X = [X[idx] for idx in idx_list]\n","        Y = [Y[idx] for idx in idx_list]\n","        \n","        #K_fold separation\n","        fold_size = len(X) // K_fold\n","        start_validation_idx = validation_set_idx * fold_size\n","        end_validation_idx = start_validation_idx + fold_size\n","        self.X_train = X[slice(0, start_validation_idx)] + X[slice(end_validation_idx, None)]\n","        self.Y_train = Y[slice(0, start_validation_idx)] + Y[slice(end_validation_idx, None)]\n","        self.X_validation = X[slice(start_validation_idx, end_validation_idx)]\n","        self.Y_validation = Y[slice(start_validation_idx, end_validation_idx)]\n","\n","        #data augmentation\n","        #self.X_train = compose_all_functions_for_data(self.X_train)\n","        #self.Y_train = compose_all_functions_for_data(self.Y_train)\n","        #self.X_validation = compose_all_functions_for_data(self.X_validation)\n","        #self.Y_validation = compose_all_functions_for_data(self.Y_validation)\n","        self.n_samples = len(self.X_train)\n","        \n","        #casting into tensors\n","        self.X_train = torch.stack(self.X_train)\n","        self.X_validation = torch.stack(self.X_validation)\n","        self.Y_train = torch.reshape(torch.stack(self.Y_train) , (-1, shape_y[1], shape_y[2]))\n","        self.Y_validation = torch.reshape(torch.stack(self.Y_validation) , (-1, shape_y[1], shape_y[2]))\n","\n","        #creating dataloader for validation data\n","        class dataset_validation(Dataset):\n","            def __init__(s,x,y):\n","                s.x = x\n","                s.y = y\n","                s.size = len(s.x)\n","            def __getitem__(s, index):\n","                return s.x[index], s.y[index]\n","            def __len__(s):\n","                return s.size\n","               \n","        self.validation_data_loader = torch.utils.data.DataLoader(\n","            dataset_validation(self.X_validation, self.Y_validation),\n","            batch_size = batch_size_val, shuffle = False)\n","        \n","        \n","    def __getitem__(self, index):\n","        return self.X_train[index], self.Y_train[index]\n","\n","    def __len__(self):\n","        return self.n_samples\n","    \n","    def get_validation_dataloader(self):\n","        return self.validation_data_loader\n","\n","\n","dataset = imagesDataset(K_FOLD, VALIDATION_SET_IDX, BATCH_SIZE_VAL, SEED)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True)\n","validation_dataloader = dataset.get_validation_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"y9vik-NkOITr"},"source":["## Model"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"iDEW3FPqOITv","executionInfo":{"status":"ok","timestamp":1639667833008,"user_tz":-60,"elapsed":366,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        \n","        # \n","        self.pool_d = nn.MaxPool2d(2, 2)\n","        self.pool_u = nn.Upsample(scale_factor=2)\n","        \n","        # Activation function\n","        self.activ = nn.ReLU()\n","        #self.activ = nn.LeakyReLU(0.1)\n","        \n","        # Convolution Downwards\n","        self.conv_1 = nn.Conv2d(3, 64, (3,3), padding=(1, 1))\n","        self.conv_2 = nn.Conv2d(64, 64, (3,3), padding=(1, 1))\n","        \n","        self.conv_3 = nn.Conv2d(64, 128, (3,3), padding=(1, 1))\n","        self.conv_4 = nn.Conv2d(128, 128, (3,3), padding=(1, 1))\n","        \n","        self.conv_5 = nn.Conv2d(128, 256, (3,3), padding=(1, 1))\n","        self.conv_6 = nn.Conv2d(256, 256, (3,3), padding=(1, 1))\n","        \n","        self.conv_7 = nn.Conv2d(256, 512, (3,3), padding=(1, 1))\n","        self.conv_8 = nn.Conv2d(512, 512, (3,3), padding=(1, 1))\n","        \n","        self.conv_9 = nn.Conv2d(512, 1024, (3,3), padding=(1, 1))\n","        self.conv_10 = nn.Conv2d(1024, 1024, (3,3), padding=(1, 1))\n","        \n","        \n","        # Upconvolution\n","        self.upconv_1 = nn.Conv2d(512+1024, 512, (3,3), padding=(1, 1))\n","        self.upconv_2 = nn.Conv2d(512, 512, (3,3), padding=(1, 1))\n","        \n","        self.upconv_3 = nn.Conv2d(256+512, 256, (3,3), padding=(1, 1))\n","        self.upconv_4 = nn.Conv2d(256, 256, (3,3), padding=(1, 1))\n","        \n","        self.upconv_5 = nn.Conv2d(128+256, 128, (3,3), padding=(1, 1))\n","        self.upconv_6 = nn.Conv2d(128, 128, (3,3), padding=(1, 1))\n","        \n","        self.upconv_7 = nn.Conv2d(64+128, 64, (3,3), padding=(1, 1))\n","        self.upconv_8 = nn.Conv2d(64, 64, (3,3), padding=(1, 1))\n","        self.upconv_9 = nn.Conv2d(64, 2, (1,1))\n","\n","\n","    def forward(self, x):\n","        # Convolution with activation and max_pooling\n","        xd_1 = self.activ(self.conv_1(x))\n","        xd_2 = self.activ(self.conv_2(xd_1))\n","    \n","        xd_3 = self.activ(self.conv_3(self.pool_d(xd_2)))\n","        xd_4 = self.activ(self.conv_4(xd_3))\n","        \n","        xd_5 = self.activ(self.conv_5(self.pool_d(xd_4)))\n","        xd_6 = self.activ(self.conv_6(xd_5))\n","        \n","        xd_7 = self.activ(self.conv_7(self.pool_d(xd_6)))\n","        xd_8 = self.activ(self.conv_8(xd_7))\n","        \n","        xd_9 = self.activ(self.conv_9(self.pool_d(xd_8)))\n","        xd_10 = self.pool_u(self.activ(self.conv_10(xd_9)))\n","\n","        # \"Fractionally / Backward strided convolution\" with activation and upsampling\n","        xu_1 = self.activ(self.upconv_1(torch.cat((xd_8, xd_10), dim=1)))\n","        xu_2 = self.pool_u(self.activ(self.upconv_2(xu_1)))\n","        \n","        xu_3 = self.activ(self.upconv_3(torch.cat((xd_6, xu_2), dim=1)))\n","        xu_4 = self.pool_u(self.activ(self.upconv_4(xu_3)))\n","        \n","        xu_5 = self.activ(self.upconv_5(torch.cat((xd_4, xu_4), dim=1)))\n","        xu_6 = self.pool_u(self.activ(self.upconv_6(xu_5)))\n","        \n","        xu_7 = self.activ(self.upconv_7(torch.cat((xd_2, xu_6), dim=1)))\n","        xu_8 = self.activ(self.upconv_8(xu_7))\n","        xu_9 = self.upconv_9(xu_8)\n","    \n","        return xu_9\n","\n","###################################################################################################################################\n","###################################################################################################################################\n","\n","class ConvNet_not_recurrent(nn.Module):\n","    def __init__(self):\n","        super(ConvNet_not_recurrent, self).__init__()\n","        self.pool_d = nn.MaxPool2d(2, 2)\n","        self.pool_u = nn.Upsample(scale_factor=2)\n","        \n","        # Activation function\n","        self.activ = nn.ReLU()\n","        #self.activ = nn.LeakyReLU(0.1)\n","        \n","        # Convolution Downwards\n","        self.conv_1 = nn.Conv2d(3, 64, (3,3), padding=(1, 1))\n","        self.conv_2 = nn.Conv2d(64, 64, (3,3), padding=(1, 1))\n","        \n","        self.conv_3 = nn.Conv2d(64, 128, (3,3), padding=(1, 1))\n","        self.conv_4 = nn.Conv2d(128, 128, (3,3), padding=(1, 1))\n","        \n","        self.conv_5 = nn.Conv2d(128, 256, (3,3), padding=(1, 1))\n","        self.conv_6 = nn.Conv2d(256, 256, (3,3), padding=(1, 1))\n","        \n","        self.conv_7 = nn.Conv2d(256, 512, (3,3), padding=(1, 1))\n","        self.conv_8 = nn.Conv2d(512, 512, (3,3), padding=(1, 1))\n","        \n","        self.conv_9 = nn.Conv2d(512, 1024, (3,3), padding=(1, 1))\n","        self.conv_10 = nn.Conv2d(1024, 1024, (3,3), padding=(1, 1))\n","        \n","        \n","        # Upconvolution\n","        self.upconv_1 = nn.Conv2d(1024, 512, (3,3), padding=(1, 1))\n","        self.upconv_2 = nn.Conv2d(512, 512, (3,3), padding=(1, 1))\n","        \n","        self.upconv_3 = nn.Conv2d(512, 256, (3,3), padding=(1, 1))\n","        self.upconv_4 = nn.Conv2d(256, 256, (3,3), padding=(1, 1))\n","        \n","        self.upconv_5 = nn.Conv2d(256, 128, (3,3), padding=(1, 1))\n","        self.upconv_6 = nn.Conv2d(128, 128, (3,3), padding=(1, 1))\n","        \n","        self.upconv_7 = nn.Conv2d(128, 64, (3,3), padding=(1, 1))\n","        self.upconv_8 = nn.Conv2d(64, 64, (3,3), padding=(1, 1))\n","        self.upconv_9 = nn.Conv2d(64, 2, (1,1))\n","\n","\n","\n","    def forward(self, x):\n","        # Convolution with activation and max_pooling\n","        xd_1 = self.activ(self.conv_1(x))\n","        xd_2 = self.activ(self.conv_2(xd_1))\n","    \n","        xd_3 = self.activ(self.conv_3(self.pool_d(xd_2)))\n","        xd_4 = self.activ(self.conv_4(xd_3))\n","        \n","        xd_5 = self.activ(self.conv_5(self.pool_d(xd_4)))\n","        xd_6 = self.activ(self.conv_6(xd_5))\n","        \n","        xd_7 = self.activ(self.conv_7(self.pool_d(xd_6)))\n","        xd_8 = self.activ(self.conv_8(xd_7))\n","        \n","        xd_9 = self.activ(self.conv_9(self.pool_d(xd_8)))\n","        xd_10 = self.pool_u(self.activ(self.conv_10(xd_9)))\n","\n","        # \"Fractionally / Backward strided convolution\" with activation and upsampling\n","        xu_1 = self.activ(self.upconv_1(xd_10))\n","        xu_2 = self.pool_u(self.activ(self.upconv_2(xu_1)))\n","        \n","        xu_3 = self.activ(self.upconv_3(xu_2))\n","        xu_4 = self.pool_u(self.activ(self.upconv_4(xu_3)))\n","        \n","        xu_5 = self.activ(self.upconv_5(xu_4))\n","        xu_6 = self.pool_u(self.activ(self.upconv_6(xu_5)))\n","        \n","        xu_7 = self.activ(self.upconv_7(xu_6))\n","        xu_8 = self.activ(self.upconv_8(xu_7))\n","        xu_9 = self.upconv_9(xu_8)\n","        return xu_9\n"]},{"cell_type":"markdown","metadata":{"id":"wlX6q7XjOIT3"},"source":["## Training"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"9pCP7s9fOIT4","executionInfo":{"status":"ok","timestamp":1639667833008,"user_tz":-60,"elapsed":4,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["def train(n_epochs, data_loader, model, optimizer, scheduler, criterion, device, checkpoint_path, best_model_path):\n","    train_loader = data_loader\n","    validation_loader = train_loader.dataset.get_validation_dataloader()\n","    f1_max = 0\n","    \n","    for epoch in range(n_epochs):\n","        loss_list = []\n","        model.train()\n","        for (data, target) in train_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_list.append(loss.item())\n","        loss_epoch = np.mean(loss_list)\n","\n","        model.eval()\n","        #computing F1 score on validation data\n","        tp, fp, fn = 0, 0, 0\n","        for (data, target) in validation_loader:\n","            data, target = data.to(device), target.to(device)\n","            with torch.no_grad():\n","                output = model(data)\n","            prediction = torch.argmax(output, dim = 1)\n","            confusions = prediction / target\n","            tp += torch.sum(confusions == 1).item()\n","            print(\"tp : \", tp)\n","            fp += torch.sum(confusions == float('inf')).item()\n","            print(\"fp : \", fp)\n","            fn += torch.sum(confusions == 0).item()\n","            print(\"fn : \", fn)            \n","        f1_score_val = 2 * tp / (2 * tp + fp + fn)\n","        \n","        if f1_score_val > f1_max:\n","            torch.save(model.state_dict(), best_model_path)\n","            f1_max = f1_score_val\n","\n","        checkpoint = {\n","            'epoch': epoch,\n","            'f1_max': f1_max,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'scheduler': scheduler.get_last_lr(),\n","        }\n","        \n","        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","\n","        #computing F1 score on training data\n","        tp, fp, fn = 0, 0, 0\n","        for (data, target) in train_loader:\n","            data, target = data.to(device), target.to(device)\n","            with torch.no_grad():\n","                output = model(data)\n","            prediction = torch.argmax(output, dim = 1)\n","            confusions = prediction / target\n","            tp += torch.sum(confusions == 1).item()\n","            fp += torch.sum(confusions == float('inf')).item()\n","            fn += torch.sum(confusions == 0).item()\n","        f1_score_train = 2 * tp / (2 * tp + fp + fn)\n","\n","        print(f\"Epoch {epoch} || Loss:{loss_epoch:.6f} || Training F1 {f1_score_train:.6f}|| Validation F1 {f1_score_val:.6f} || Learning rate {scheduler.get_last_lr()[0]:.6f}\"+\"\\n\")  \n","        scheduler.step()\n","        \n","    return "]},{"cell_type":"code","execution_count":14,"metadata":{"id":"DQ5ylJbaEWjP","executionInfo":{"status":"ok","timestamp":1639667868702,"user_tz":-60,"elapsed":595,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[],"source":["LOAD_LAST_CHECKPOINT = False\n","LOAD_BEST_MODEL = False\n","\n","#model = ConvNet().to(device)\n","model = ConvNet_not_recurrent().to(device)\n","\n","if LOAD_LAST_CHECKPOINT and not LOAD_BEST_MODEL:\n","    checkpoint = torch.load(CHECKPOINT_PATH)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    \n","if not LOAD_LAST_CHECKPOINT and LOAD_BEST_MODEL:\n","    model.load_state_dict(torch.load(BEST_MODEL_PATH))\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","scheduler = ExponentialLR(optimizer, GAMMA)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFfOGWcFhQLC","outputId":"e9a1306e-24ac-4fa0-90d6-894f039856c3","executionInfo":{"status":"ok","timestamp":1639667917620,"user_tz":-60,"elapsed":46032,"user":{"displayName":"sami ferchiou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02970809549726737451"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tp :  0\n","fp :  0\n","fn :  147493\n","tp :  0\n","fp :  0\n","fn :  299768\n","tp :  0\n","fp :  0\n","fn :  461833\n","tp :  0\n","fp :  0\n","fn :  633557\n","tp :  0\n","fp :  0\n","fn :  793231\n","Epoch 0 || Loss:0.706831 || Training F1 0.000000|| Validation F1 0.000000 || Learning rate 0.001000\n","\n","tp :  0\n","fp :  0\n","fn :  147493\n","tp :  0\n","fp :  0\n","fn :  299768\n","tp :  0\n","fp :  0\n","fn :  461833\n","tp :  0\n","fp :  0\n","fn :  633557\n","tp :  0\n","fp :  0\n","fn :  793231\n","Epoch 1 || Loss:0.535013 || Training F1 0.000000|| Validation F1 0.000000 || Learning rate 0.001000\n","\n","tp :  0\n","fp :  0\n","fn :  147493\n","tp :  0\n","fp :  0\n","fn :  299768\n","tp :  0\n","fp :  0\n","fn :  461833\n","tp :  0\n","fp :  0\n","fn :  633557\n","tp :  0\n","fp :  0\n","fn :  793231\n","Epoch 2 || Loss:0.513717 || Training F1 0.000000|| Validation F1 0.000000 || Learning rate 0.001000\n","\n","tp :  0\n","fp :  0\n","fn :  147493\n","tp :  0\n","fp :  0\n","fn :  299768\n","tp :  0\n","fp :  0\n","fn :  461833\n","tp :  0\n","fp :  0\n","fn :  633557\n","tp :  0\n","fp :  0\n","fn :  793231\n","Epoch 3 || Loss:0.508713 || Training F1 0.000000|| Validation F1 0.000000 || Learning rate 0.001000\n","\n","tp :  0\n","fp :  0\n","fn :  147493\n","tp :  0\n","fp :  0\n","fn :  299768\n","tp :  0\n","fp :  0\n","fn :  461833\n","tp :  0\n","fp :  0\n","fn :  633557\n","tp :  0\n","fp :  0\n","fn :  793231\n","Epoch 4 || Loss:0.506351 || Training F1 0.000000|| Validation F1 0.000000 || Learning rate 0.001000\n","\n"]}],"source":["train(NBR_EPOCHS, train_loader, model, optimizer, scheduler, criterion, device, CHECKPOINT_PATH, BEST_MODEL_PATH)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"road_segmentation_sami.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}